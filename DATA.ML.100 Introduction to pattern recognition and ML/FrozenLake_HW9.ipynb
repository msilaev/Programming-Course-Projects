{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcUl6qGEsYtGklWurJT+pK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msilaev/ML-PatternRecCourse/blob/main/FrozenLake_HW9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is a homework assignment in ML course. The task is to find manually an optimal Q table for the Frozen Lake game in the slippery case. There is a good tutorial on this topic https://youtu.be/Vrro7W7iW2w"
      ],
      "metadata": {
        "id": "o0OWCaO1iYA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rAPNbPD9iVoj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_policy(qtable_, num_of_episodes_, max_steps_):\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_of_episodes_): # This is out loop over num of episodes\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps_):\n",
        "            action = np.argmax(qtable_[state,:])\n",
        "            new_state, reward, done, truncated = env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "            else:\n",
        "                state = new_state\n",
        "        rewards.append(total_reward)\n",
        "        env.close()\n",
        "    return sum(rewards)/num_of_episodes_"
      ],
      "metadata": {
        "id": "MQDVm0FcePN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f01ab2-8b28-4d1f-c68b-05de5dc3b9df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"ansi\")\n",
        "\n",
        "action_size = env.action_space.n\n",
        "print(\"Action size: \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State size: \", state_size)\n"
      ],
      "metadata": {
        "id": "wyYkf4BoeTeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c15006f7-b20c-470e-89be-5e570996a54b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action size:  4\n",
            "State size:  16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In general it can be seen that less \"dangerous\" way is 0-> 4-> 8 -> 9 -> 13 -> 14 -> 15.  We dont't want to go from 0 to 1, therefore we should command to go from 0 to the left. Let's check the probabilities of actions in this case. Below the command is env.P[state][action], where action = 0,1,2,3 corresponding to \"left\", \"down\", \"right\", \"up\"."
      ],
      "metadata": {
        "id": "dxkrQmZFZexv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.P[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ52H2u-cJwC",
        "outputId": "ce519dd6-060c-4218-ec40-d5854575b016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We see that agent either stays in 0 or moves to 4. The same trick can be done with other states to avoid drawning. The resulting Q table is below. One can check that the sucess probability is almost the same as with the brute force Monte Carlo solution."
      ],
      "metadata": {
        "id": "-toVEcBddl3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qtable_1 = np.zeros([state_size, action_size])\n",
        "# start(0) -> go left (0)\n",
        "qtable_1[0,0] = 1\n",
        "# state(4) -> go left (0)\n",
        "qtable_1[4,0] = 1\n",
        "# state(8) -> go right (2)\n",
        "qtable_1[8,2] = 1\n",
        "# state(9) -> go down (1)\n",
        "qtable_1[9,1] = 1\n",
        "# state(13) -> go right (2)\n",
        "qtable_1[13,2] = 1\n",
        "# state(10) -> go left (0)\n",
        "qtable_1[10,0] = 1\n",
        "\n",
        "# state 1 -> go up (3)\n",
        "qtable_1[1,3] = 1\n",
        "\n",
        "# state 1 -> go left (0)\n",
        "#qtable[1,0] = 1\n",
        "\n",
        "# state 2 -> go up (3)\n",
        "qtable_1[2,3] = 1\n",
        "# state 3 -> go up (3)\n",
        "qtable_1[3,3] = 1\n",
        "# state 6 -> go down (1)\n",
        "qtable_1[6,1] = 1\n",
        "# state 10 -> go left (0)\n",
        "qtable_1[10,0] = 1\n",
        "# state 14 -> go down (1) -> GOAL or 13\n",
        "qtable_1[14,1] = 1\n",
        "\n",
        "\n",
        "qtable_1 = np.zeros([state_size, action_size])\n",
        "qtable_1[0,0] = 1\n",
        "qtable_1[1,3] = 1\n",
        "qtable_1[2,3] = 1\n",
        "qtable_1[3,3] = 1\n",
        "qtable_1[4,0] = 1\n",
        "\n",
        "qtable_1[6,1] = 1\n",
        "qtable_1[8,2] = 1\n",
        "\n",
        "qtable_1[9,1] = 1\n",
        "qtable_1[10,0] = 1\n",
        "qtable_1[13,2] = 1\n",
        "qtable_1[14,1] = 1\n"
      ],
      "metadata": {
        "id": "kZ0Cfr4DVUvC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(qtable)\n",
        "num_of_episodes_ = 10000\n",
        "print(f'Total reward for manually and \"smartly\" defined Q-table: {eval_policy(qtable_1, num_of_episodes_, 100)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIrBa6YeYpmF",
        "outputId": "e62526ac-c696-4a42-e4a0-61ca24f76ab1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward for manually and \"smartly\" defined Q-table: 0.2223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional optimization is to change the action at states 6,8 to \"up\" (3)."
      ],
      "metadata": {
        "id": "hVo7tyi04JE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qtable_olli = np.zeros([state_size, action_size])\n",
        "qtable_olli[0,0] = 1\n",
        "qtable_olli[1,3] = 1\n",
        "qtable_olli[2,3] = 1\n",
        "qtable_olli[3,3] = 1\n",
        "qtable_olli[4,0] = 1\n",
        "\n",
        "qtable_olli[6,3] = 1\n",
        "qtable_olli[8,3] = 1\n",
        "\n",
        "qtable_olli[9,1] = 1\n",
        "qtable_olli[10,0] = 1\n",
        "qtable_olli[13,2] = 1\n",
        "qtable_olli[14,1] = 1\n",
        "#print(qtable)"
      ],
      "metadata": {
        "id": "5b2oavTEe3VJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(qtable)\n",
        "num_of_episodes_ = 10000\n",
        "print(f'Total reward for manually and \"smartly\" defined Q-table: {eval_policy(qtable_olli, num_of_episodes_, 100)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqzewaND77_0",
        "outputId": "e324e6ce-535a-4dfb-9e13-27b782943b50"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward for manually and \"smartly\" defined Q-table: 0.6582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gWZIIgAx7-Wr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}